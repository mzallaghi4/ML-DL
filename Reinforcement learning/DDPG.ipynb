{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30cec190-3a71-4fec-b719-564cb3bb0826",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "DDPG is an off-policy, model-free reinforcement learning algorithm designed for continuous action spaces. \n",
    "\n",
    "It combines ideas from DQN (value-based) and policy gradients (actor-critic methods), and was introduced by DeepMind in 2015.\n",
    "\n",
    "Actor Network (μ): Learns the deterministic policy\n",
    " - Maps state → continuous action\n",
    "\n",
    "Critic Network (Q): Learns the action-value function\n",
    " - Maps (state, action) → expected return\n",
    "\n",
    "\n",
    "##### Implementation of DDPG for stock portfolio optimization via Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "408a50d2-c894-426c-ac47-9d8ba42694bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c107f58-3b45-4948-ad10-4dab3ed5d452",
   "metadata": {},
   "source": [
    "##### Define the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a79ec3d-21cb-43c9-82c3-40357fb445df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnvironment:\n",
    "    def __init__(self, data, initial_balance=100000, transaction_cost=0.001):\n",
    "        self.data = data\n",
    "        self.initial_balance = initial_balance\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.n_assets = len(data.columns)  \n",
    "        self.current_step = 0\n",
    "        self.balance = initial_balance\n",
    "        self.shares_held = np.zeros(self.n_assets)\n",
    "        self.portfolio_value = initial_balance\n",
    "        self.previous_portfolio_value = initial_balance      \n",
    "        self.prices = data.values\n",
    "        self.n_steps = len(data)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.shares_held = np.zeros(self.n_assets)\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.previous_portfolio_value = self.initial_balance\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        current_prices = self.prices[self.current_step]\n",
    "        lookback = min(10, self.current_step + 1)\n",
    "        price_history = self.prices[self.current_step - lookback + 1:self.current_step + 1]\n",
    "        if len(price_history) >= 5:\n",
    "            returns = np.diff(price_history, axis=0) / price_history[:-1]\n",
    "            mean_returns = np.mean(returns, axis=0)\n",
    "            volatility = np.std(returns, axis=0)\n",
    "        else:\n",
    "            mean_returns = np.zeros(self.n_assets)\n",
    "            volatility = np.ones(self.n_assets)\n",
    "\n",
    "        portfolio_weights = self._get_portfolio_weights()\n",
    "        portfolio_return = (self.portfolio_value - self.previous_portfolio_value) / self.previous_portfolio_value\n",
    "\n",
    "        state = np.concatenate([\n",
    "            current_prices / current_prices[0],  # Normalized prices\n",
    "            mean_returns,\n",
    "            volatility,\n",
    "            portfolio_weights,\n",
    "            [self.balance / self.portfolio_value],  # Cash ratio\n",
    "            [portfolio_return]  # Portfolio return\n",
    "        ])\n",
    "        \n",
    "        return state.astype(np.float32)\n",
    "    \n",
    "    def _get_portfolio_weights(self):\n",
    "        current_prices = self.prices[self.current_step]\n",
    "        asset_values = self.shares_held * current_prices\n",
    "        total_value = self.portfolio_value\n",
    "        \n",
    "        if total_value > 0:\n",
    "            weights = asset_values / total_value\n",
    "        else:\n",
    "            weights = np.zeros(self.n_assets)\n",
    "            \n",
    "        return weights\n",
    "    \n",
    "    def _calculate_transaction_cost(self, old_weights, new_weights):\n",
    "        total_value = self.portfolio_value\n",
    "        cost = np.sum(np.abs(new_weights - old_weights)) * total_value * self.transaction_cost\n",
    "        return cost\n",
    "    \n",
    "    def step(self, action):\n",
    "        action = np.clip(action, 0, 1)  # Ensure non-negative weights\n",
    "        action = action / (np.sum(action) + 1e-8)  # Normalize to sum to 1\n",
    "        current_prices = self.prices[self.current_step]\n",
    "        old_weights = self._get_portfolio_weights()\n",
    "        transaction_cost = self._calculate_transaction_cost(old_weights, action)\n",
    "        total_value = self.portfolio_value - transaction_cost\n",
    "        target_values = action * total_value\n",
    "        target_shares = target_values / (current_prices + 1e-8)\n",
    "        self.shares_held = target_shares\n",
    "        self.balance = 0  # All money invested in assets\n",
    "        self.previous_portfolio_value = self.portfolio_value\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Calculate new portfolio value\n",
    "        if self.current_step < self.n_steps:\n",
    "            next_prices = self.prices[self.current_step]\n",
    "            self.portfolio_value = np.sum(self.shares_held * next_prices)\n",
    "        else:\n",
    "            self.portfolio_value = np.sum(self.shares_held * current_prices)\n",
    "        \n",
    "        # Calculate reward\n",
    "        portfolio_return = (self.portfolio_value - self.previous_portfolio_value) / self.previous_portfolio_value\n",
    "        reward = portfolio_return\n",
    "        \n",
    "        # Add penalty for high volatility\n",
    "        if self.current_step > 1:\n",
    "            returns_history = []\n",
    "            start_idx = max(0, self.current_step - 30)  # Last 30 days\n",
    "            for i in range(start_idx, self.current_step):\n",
    "                if i > 0:\n",
    "                    prev_val = np.sum(self.shares_held * self.prices[i-1])\n",
    "                    curr_val = np.sum(self.shares_held * self.prices[i])\n",
    "                    if prev_val > 0:\n",
    "                        ret = (curr_val - prev_val) / prev_val\n",
    "                        returns_history.append(ret)\n",
    "            \n",
    "            if len(returns_history) > 1:\n",
    "                volatility_penalty = -np.std(returns_history) * 0.1\n",
    "                reward += volatility_penalty\n",
    "        \n",
    "        # Check if done\n",
    "        done = self.current_step >= self.n_steps - 1\n",
    "        \n",
    "        # Get next state\n",
    "        if not done:\n",
    "            next_state = self._get_state()\n",
    "        else:\n",
    "            next_state = np.zeros_like(self._get_state())\n",
    "        \n",
    "        return next_state, reward, done, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7621e6-d7be-4e48-b235-4bc919baeea6",
   "metadata": {},
   "source": [
    "##### Define Neural Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c61e18-b1e6-4a5e-99ec-ad07e86eaac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d77b776-87b9-482d-8b65-ee7979bbcd3d",
   "metadata": {},
   "source": [
    "#### Implement DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0ad1bf-a2c9-4d83-9c50-4c83f1ffb1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e82d1440-ae7c-4bc1-87ed-60498a397e4e",
   "metadata": {},
   "source": [
    "#### Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe98775-3659-4e26-9fda-a119444a2cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6896a892-9a5d-41c8-9113-bf48a6856e59",
   "metadata": {},
   "source": [
    "##### Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f85a522-c52c-4977-b39b-b2bc1689d943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
